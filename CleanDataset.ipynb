{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CleanDataset.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Better Together: Dataset Pre-processing\n","This notebook is used to process the data for node embedding creation. Simply run the cells below, making sure to follow the written instructions!\n","\n","## Source\n","https://snap.stanford.edu/data/cit-HepTh.html\n","\n","## Data statistics\n","- Number of edges: 352808\n","- Number of valid edges: 352807\n","- Number of nodes: 27770 (documents)"],"metadata":{"id":"D1jDyMKWiMBB"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJgXVqu1xg6u","outputId":"766de602-6eaa-4d03-90c4-631225822b73"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Only uncomment this cell if you plan on using Google Colab\n","#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Before running this notebook, make sure to set the project directory below to the location of where you unzipped the dataset files."],"metadata":{"id":"jwcKayl6VRbS"}},{"cell_type":"code","source":["project_dir = 'drive/MyDrive/GaTechHw/CSE6240/Project/citationDataset/'"],"metadata":{"id":"0SFAcaK7yW4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["node_fp = 'Cit-HepTh.txt'"],"metadata":{"id":"Lnq1utyACS3f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import Packages"],"metadata":{"id":"TaxXzkIQVOVn"}},{"cell_type":"code","source":["import io\n","import os\n","import pandas as pd"],"metadata":{"id":"y812q28eywJm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Text File Processing\n","\n","The function below extracts text from a single .abs file."],"metadata":{"id":"ojDuUsFRQxdY"}},{"cell_type":"code","source":["def extract_text(file):\n","    lines = file.split('\\n')\n","    sep_count = 0\n","    text = ''\n","    for i in range(len(lines)):\n","        if lines[i] == '\\\\\\\\':\n","            sep_count += 1\n","            continue\n","        if sep_count == 2:\n","            text += lines[i] + ' '\n","    text = str.strip(text)\n","    return text"],"metadata":{"id":"rqXdoqCN0Tuh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set the ```abs_folder``` variable below to the folder that contains the extract data from the [SNAP Website](https://snap.stanford.edu/data/cit-HepTh.html). The ```years``` variable corresponds to the folder containing abstracts for each year in the datset."],"metadata":{"id":"p8V3S5u7TKlb"}},{"cell_type":"code","source":["abs_folder = 'cit-HepTh-abstracts/'"],"metadata":{"id":"JpzkYqaNyd5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["years = ['1992','1993','1994','1995', '1996','1997','1998','1999',\n","         '2000','2001','2002','2003']"],"metadata":{"id":"RyuTQVyqjO0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And set the ```save_fp``` variable below to the file name you would like for the text file containing the line separated abstract data."],"metadata":{"id":"pqrscd-fXNBP"}},{"cell_type":"code","source":["save_fp = 'abs_text.txt'"],"metadata":{"id":"nj5ih44rkD-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_list = []\n","node_file_list = []\n","\n","for folder in years:\n","    file_list = os.listdir(project_dir + abs_folder + folder + '/')\n","\n","    for file in file_list:\n","        fp = project_dir + abs_folder + folder + '/' + file\n","        with open(fp, \"r\", encoding=\"utf-8\") as f:\n","            file_text = f.read()\n","            extracted_text = extract_text(file_text)\n","            text_list.append(extracted_text)\n","    \n","    for file in file_list:\n","        node = file[:-4]\n","        node_file_list.append(node)"],"metadata":{"id":"C9-P1z6Uzn3S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(save_fp, \"w\", encoding=\"utf-8\") as f:\n","    for i in range(len(text_list)):\n","        f.write(node_file_list[i] + '\\t' + text_list[i] + '\\n')"],"metadata":{"id":"dVUqunDDvRPJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You should now see a saved file with the file path set above containing the cleaned abstract text data for each paper/node!"],"metadata":{"id":"vPr_kMhGY_--"}}]}